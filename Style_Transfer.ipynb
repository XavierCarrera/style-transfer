{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Style Transfer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPfyelZtqMYRj8SmbKjLZM9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XavierCarrera/style-transfer/blob/main/Style_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJ0gIIBxPs08",
        "outputId": "206079d3-3db3-4a58-ec7a-a7534427ed9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gkq0CT35XCoy",
        "outputId": "ee20c90b-8f5f-4369-fd32-4172b66be864",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd \"/content/drive/My Drive\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_oOJKjFYXkF",
        "outputId": "755016d0-5bb5-476c-8298-5aae420e80e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/aitorzip/PyTorch-CycleGAN.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'PyTorch-CycleGAN'...\n",
            "remote: Enumerating objects: 60, done.\u001b[K\n",
            "remote: Total 60 (delta 0), reused 0 (delta 0), pack-reused 60\u001b[K\n",
            "Unpacking objects: 100% (60/60), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfT0fOvsDB77",
        "outputId": "8dbcf997-bb92-44fc-fdbc-0a2edd415e2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd '/content/drive/My Drive/PyTorch-CycleGAN'"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/PyTorch-CycleGAN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIoNgS58DOm8"
      },
      "source": [
        "%%sh\n",
        "sh ./download_dataset summer2winter_yosemite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYvVDr6PDYyN"
      },
      "source": [
        "!mv datasets/summer2winter_yosemite /content/drive/My Drive/dl-pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qhh4kvGIGI5h"
      },
      "source": [
        "# Definition of Generative Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyjnOfEeF_SJ"
      },
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self, in_features):\n",
        "    super(ResidualBlock, self).__init__()\n",
        "    \n",
        "    conv_block = [ nn.ReflectionPad2d(1), \n",
        "                   nn.Conv2d(in_features, in_features, 3),\n",
        "                   nn.InstanceNorm2d(in_features), \n",
        "                   nn.ReLU(True),\n",
        "                   nn.ReflectionPad2d(1), #\n",
        "                   nn.Conv2d(in_features, in_features, 3),\n",
        "                   nn.InstanceNorm2d(in_features)\n",
        "                 ]\n",
        "    \n",
        "    self.conv_block = nn.Sequential(*conv_block)\n",
        "  def forward(self, x):\n",
        "    return self.conv_block(x) + x "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fumswxJbGE6E"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, input_nc, output_nc, n_residual_blocks=9):\n",
        "    super(Generator,self).__init__()\n",
        "    \n",
        "    model = [ nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(input_nc, 64, 7), # I - 7 + 6 /1 +1 = I\n",
        "            nn.InstanceNorm2d(64),\n",
        "             nn.ReLU(True)\n",
        "            ]\n",
        "    \n",
        "    in_features = 64\n",
        "    out_features = in_features * 2\n",
        "    \n",
        "    for _ in range(2):\n",
        "      model += [ nn.Conv2d(in_features, out_features, 3, stride=2, padding=1), #I/2\n",
        "                 nn.InstanceNorm2d(out_features),\n",
        "                 nn.ReLU(True)\n",
        "               ]\n",
        "      in_features = out_features\n",
        "      out_features = in_features*2\n",
        "    \n",
        "    for _ in range(n_residual_blocks):\n",
        "      model += [ResidualBlock(in_features)]\n",
        "    \n",
        "    \n",
        "    out_features = in_features// 2\n",
        "    for _ in range(2):\n",
        "      model += [ nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1), #2I\n",
        "                 nn.InstanceNorm2d(out_features),\n",
        "                 nn.ReLU(inplace=True)\n",
        "               ]\n",
        "      in_features = out_features\n",
        "      out_features = in_features //2\n",
        "  \n",
        "    model += [ nn.ReflectionPad2d(3),\n",
        "               nn.Conv2d(64, output_nc, 7), \n",
        "               nn.Tanh()\n",
        "             ]\n",
        "      \n",
        "    self.model = nn.Sequential(*model)\n",
        "      \n",
        "  def forward(self,x):\n",
        "    return self.model(x)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C2_NNTFGkXP"
      },
      "source": [
        "# Discriminative Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_6_23edGgvy"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "  \"PatchGAN: discrimina estilo o textura\"\n",
        "  def __init__(self, input_nc):\n",
        "    super(Discriminator, self).__init__()\n",
        "    \n",
        "    model = [ nn.Conv2d(input_nc, 64, 4, stride=2, padding=1), #I/2\n",
        "              nn.LeakyReLU(0.2, inplace=True)\n",
        "            ]\n",
        "    \n",
        "    model += [ nn.Conv2d(64, 128, 4, stride=2, padding=1), #I/2\n",
        "               nn.InstanceNorm2d(128),\n",
        "              nn.LeakyReLU(0.2, inplace=True)\n",
        "             ]\n",
        "    \n",
        "    model += [ nn.Conv2d(128, 256, 4, stride=2, padding=1), #I/2\n",
        "               nn.InstanceNorm2d(256),\n",
        "              nn.LeakyReLU(0.2, inplace=True)\n",
        "             ]\n",
        "    \n",
        "    model += [ nn.Conv2d(256, 512, 4, padding=1), #I-1\n",
        "               nn.InstanceNorm2d(512),\n",
        "              nn.LeakyReLU(0.2, inplace=True)\n",
        "             ]\n",
        "    \n",
        "    # Flatten\n",
        "    model += [nn.Conv2d(512, 1, 4, padding=1)] #I-1\n",
        "    \n",
        "    self.model = nn.Sequential(*model)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.model(x)\n",
        "    return F.avg_pool2d(x, x.size()[2:]).view(x.size()[0], -1)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5skRtVQHEiu"
      },
      "source": [
        "# Training Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhOLJStCGxNW"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/dl-pytorch')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSZf0LjTKfxW"
      },
      "source": [
        "!pip install visdom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFvxjOGmHRA9"
      },
      "source": [
        "import glob\n",
        "import random\n",
        "import os\n",
        "import itertools\n",
        "from PIL import Image\n",
        "\n",
        "import torch \n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from utils import ReplayBuffer"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1zDSBdEHfe9"
      },
      "source": [
        "class ImageDataset(Dataset):\n",
        "  def __init__(self, base_dir, transform=None, split='train'):\n",
        "    self.transform = transforms.Compose(transform)\n",
        "    self.files_A = sorted(glob.glob(os.path.join(base_dir, '{}/A/*.*'.format(split))))\n",
        "    self.files_B = sorted(glob.glob(os.path.join(base_dir, '{}/B/*.*'.format(split))))\n",
        "    \n",
        "  def __len__(self):\n",
        "    return max(len(self.files_A), len(self.files_B))\n",
        "  \n",
        "  def __getitem__(self,idx):\n",
        "    image_A = self.transform(Image.open(self.files_A[idx]))\n",
        "    image_B = self.transform(Image.open(self.files_B[random.randint(0,len(self.files_B)-1)]))\n",
        "    return {'A': image_A, 'B': image_B}"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqMys7zsHpnV"
      },
      "source": [
        "# Network Instances and Losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWgPqZEWHyeE"
      },
      "source": [
        "epoch = 0\n",
        "n_epochs = 200\n",
        "batch_size = 2\n",
        "lr = 0.0002\n",
        "size = 256\n",
        "input_nc = 3\n",
        "output_nc = 3\n",
        "decay_epoch = 100 #pending\n",
        "\n",
        "cuda = True\n",
        "n_cpu = 8\n",
        "\n",
        "base_dir = '/content/drive/My Drive/dl-pytorch/datasets/datasets/summer2winter_yosemite'"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOmLLwehIGG3",
        "outputId": "56960f57-23c1-4393-f688-b705cd80dc09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "device = torch.device('cuda' if cuda else 'cpu')\n",
        "\n",
        "def weights_init_normal(m):\n",
        "  if isinstance(m, nn.Conv2d):\n",
        "    torch.nn.init.normal(m.weight.data, 0.0, 0.02)\n",
        "  elif isinstance(m, nn.BatchNorm2d):\n",
        "    torch.nn.init.normal(m.weight.data, 1.0, 0.02)\n",
        "    torch.nn.init.constant(m.bias, 0.0)\n",
        "    \n",
        "netG_A2B = Generator(input_nc, output_nc)\n",
        "netG_B2A = Generator(input_nc, output_nc)\n",
        "netD_A = Discriminator(input_nc)\n",
        "netD_B = Discriminator(input_nc)\n",
        "\n",
        "netG_A2B.apply(weights_init_normal)\n",
        "netG_B2A.apply(weights_init_normal)\n",
        "netD_A.apply(weights_init_normal)\n",
        "netD_B.apply(weights_init_normal)\n",
        "\n",
        "if cuda:\n",
        "  netG_A2B.to(device)\n",
        "  netG_B2A.to(device)\n",
        "  netD_A.to(device)\n",
        "  netD_B.to(device)\n",
        "  \n",
        "criterion_GAN = torch.nn.MSELoss()\n",
        "criterion_cycle = torch.nn.L1Loss()\n",
        "criterion_identity = torch.nn.L1Loss()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gXtJmTkOBvz"
      },
      "source": [
        "# Optimizers and Schedulers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bdL5RJXMim9"
      },
      "source": [
        "optimizer_G = torch.optim.Adam(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()),\n",
        "                              lr=lr, betas=(0.5, 0.999))\n",
        "optimizer_D_A = torch.optim.Adam(netD_A.parameters(), lr=lr, betas=(0.5,0.999))\n",
        "optimizer_D_B = torch.optim.Adam(netD_B.parameters(), lr=lr, betas=(0.5,0.999))\n",
        "\n",
        "\n",
        "class LambdaLR():\n",
        "  def __init__(self, n_epochs, offset, decay_start_epoch):\n",
        "    assert ((n_epochs - decay_start_epoch) > 0)\n",
        "    self.n_epochs = n_epochs\n",
        "    self.offset = offset\n",
        "    self.decay_start_epoch = decay_start_epoch\n",
        "    \n",
        "  def step(self, epoch):\n",
        "    return 1 - max(0, epoch + self.offset - self.decay_start_epoch)/(self.n_epochs - self.decay_start_epoch)\n",
        "\n",
        "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=LambdaLR(n_epochs,epoch,decay_epoch).step)\n",
        "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=LambdaLR(n_epochs,epoch,decay_epoch).step)\n",
        "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=LambdaLR(n_epochs,epoch,decay_epoch).step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6UWLi-gOPmV"
      },
      "source": [
        "# Loss Calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQc05nO7ONzG"
      },
      "source": [
        "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
        "target_real = Tensor(batch_size).fill_(1.0)\n",
        "target_fake = Tensor(batch_size).fill_(0.0)\n",
        "\n",
        "fake_A_buffer = ReplayBuffer()\n",
        "fake_B_buffer = ReplayBuffer()\n",
        "\n",
        "transform = [ transforms.Resize(int(size*1.12), Image.BICUBIC),\n",
        "              transforms.RandomCrop(size),\n",
        "             transforms.RandomHorizontalFlip(),\n",
        "             transforms.ToTensor(),\n",
        "             transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "            ]\n",
        "\n",
        "dataloader = DataLoader(ImageDataset(base_dir,transform=transform),\n",
        "                       batch_size=batch_size, shuffle=True, num_workers=n_cpu, drop_last=True)\n",
        "\n",
        "def Gen_GAN_loss(G, D, real, loss, target_real):\n",
        "  fake = G(real)\n",
        "  pred_fake = D(fake)\n",
        "  L = loss(pred_fake, target_real)\n",
        "  return L, fake\n",
        "\n",
        "def cycle_loss(G1, G2, real, loss):\n",
        "  recovered = G2(G1(real))\n",
        "  L = loss(recovered, real)\n",
        "  return L\n",
        "\n",
        "def identity_loss(G, real, loss):\n",
        "  same = G(real)\n",
        "  L = loss(same,real)\n",
        "  return L\n",
        "\n",
        "def Disc_GAN_loss(D2, fake2, real2, fake_2_buffer, loss, target_real, target_fake):\n",
        "  pred_real = D2(real2)\n",
        "  loss_D2_real = loss(pred_real, target_real)\n",
        "  \n",
        "  fake2 = fake_2_buffer.push_and_pop(fake2)\n",
        "  pred_fake = D2(fake2.detach())\n",
        "  loss_D2_fake = loss(pred_fake, target_fake)\n",
        "  loss_D2 = (loss_D2_real + loss_D2_fake) * 0.5\n",
        "  return loss_D2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_LUCKRbOZxJ"
      },
      "source": [
        "# Training and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__eHqGLCOf-8"
      },
      "source": [
        "!pip install livelossplot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNayFlMbOi6U"
      },
      "source": [
        "from livelossplot import PlotLosses\n",
        "from utils import Logger\n",
        "\n",
        "logger = Logger(n_epochs, len(dataloader), epoch=epoch)\n",
        "liveloss= PlotLosses()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvCtaIMROlzE"
      },
      "source": [
        "for epoch in range(epoch, n_epochs):\n",
        "  for i, batch in enumerate(dataloader):\n",
        "    real_A = batch['A'].to(device)\n",
        "    real_B = batch['B'].to(device)\n",
        "    \n",
        "    # Generative Networks\n",
        "    optimizer_G.zero_grad()\n",
        "    \n",
        "    loss_GAN_A2B, fake_B = Gen_GAN_loss(netG_A2B, netD_B, real_A, criterion_GAN, target_real)\n",
        "    loss_GAN_B2A, fake_A = Gen_GAN_loss(netG_B2A, netD_A, real_B, criterion_GAN, target_real)\n",
        "    \n",
        "    loss_cycle_ABA = cycle_loss(netG_A2B, netG_B2A, real_A, criterion_cycle)\n",
        "    loss_cycle_BAB = cycle_loss(netG_B2A, netG_A2B, real_B, criterion_cycle)\n",
        "    \n",
        "    loss_identity_A = identity_loss(netG_B2A, real_A, criterion_identity)\n",
        "    loss_identity_B = identity_loss(netG_A2B, real_B, criterion_identity)\n",
        "    \n",
        "    loss_G = (loss_GAN_A2B + loss_GAN_B2A) + 10.0*(loss_cycle_ABA + loss_cycle_BAB) + 5.0 *(loss_identity_A + loss_identity_B)\n",
        "    loss_G.backward()\n",
        "    \n",
        "    optimizer_G.step()\n",
        "    \n",
        "    # Discriminative Networks\n",
        "    optimizer_D_A.zero_grad()\n",
        "    \n",
        "    loss_D_A = Disc_GAN_loss(netD_A, fake_A, real_A, fake_A_buffer, criterion_GAN, target_real, target_fake)\n",
        "    loss_D_A.backward()\n",
        "    optimizer_D_A.step()\n",
        "    \n",
        "    optimizer_D_B.zero_grad()\n",
        "    \n",
        "    loss_D_B = Disc_GAN_loss(netD_B, fake_B, real_B, fake_B_buffer, criterion_GAN, target_real, target_fake)\n",
        "    loss_D_B.backward()\n",
        "    optimizer_D_B.step()\n",
        "    \n",
        "    log_values = {'loss_G': loss_G,\n",
        "                 'loss_G_identity': (loss_identity_A + loss_identity_B),\n",
        "                 'loss_G_GAN': (loss_GAN_A2B + loss_GAN_B2A),\n",
        "                 'loss_G_cyle': loss_cycle_ABA + loss_cycle_BAB,\n",
        "                  'loss_D': loss_D_A + loss_D_B\n",
        "                 }\n",
        "    logger.log(log_values, images={'real_A': real_A,'real_B': real_B, 'fake_A': fake_A,'fake_B': fake_B  })\n",
        "  \n",
        "  liveloss.update(log_values)\n",
        "  liveloss.draw()\n",
        "    \n",
        "  lr_scheduler_G.step()\n",
        "  lr_scheduler_D_A.step()\n",
        "  lr_scheduler_D_B.step()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}